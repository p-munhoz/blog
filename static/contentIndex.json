{"airbyte/how-to-backup-airbyte-gcp":{"title":"How to backup Airbyte on Google Cloud Storage?","links":["Airbyte"],"tags":["Airbyte","GCP"],"content":"I found on the airbyte forum how to backup the Airbyte instance to a new instance but I also used this resource to create a backup script of Airbyte.\nFor a migration from one place to another\nBackup creation of the old instance\n\nStop all the services\n\ndocker-compose down\n\nStart only the database\n\ndocker-compose up -d db\n\nCreate the backup database file\n\ndocker exec airbyte-db pg_dump -U docker airbyte &gt; airbyte_backup.sql\n\nStop the service again\n\ndocker-compose down\nWhen you‚Äôve done that you have generated airbyte_backup.sql file.\nBackup load on the new instance\nGo to your new server and start Airbyte using¬†docker-compose up -d¬†after the service is running let‚Äôs stop to rebuild the database with¬†docker-compose down.\n\nStart only the database\n\ndocker-compose up -d db\n\nDrop the database created by Airbyte by default\n\ndocker exec airbyte-db psql -U docker -c &#039;drop database airbyte&#039;;\n\nCreate the database to load the backup\n\n`docker exec airbyte-db psql -U docker -c &#039;create database airbyte with owner docker;&#039;`\n\nThen regenerate the airbyte_backup.sql into the airbyte database.\n\ncat airbyte_backup.sql| docker exec -i airbyte-db psql -U docker -d airbyte\n\nThen stop the database\n\ndocker-compose up -d\nPlease make sure to have put the backup file in the folder of your new Airbyte instance.\nBackup script on Google Cloud Storage\nI created this bash script in order to generate a airbyte_backup.sql file and load it into a GCS bucket.\n#!/bin/bash\n \n# Get the current date \ncurrent_date=$(date +%F)\nbucket_name=&quot;your_bucket_name&quot;\n \n# Stop all services\n \necho &quot;Stopping all services...&quot;\n \ndocker compose down\n \n  \n \n# Start only the database\n \necho &quot;Starting only the database...&quot;\n \ndocker compose up -d db\n \n  \n \n# Create the backup database file\n \necho &quot;Creating the backup database file...&quot;\n \ndocker exec airbyte-db pg_dump -U docker airbyte &gt; airbyte_backup.sql\n \n  \n \n# Stop the service again\n \necho &quot;Stopping all services again...&quot;\n \ndocker compose down\n \n# Upload the generated .sql to Google Cloud Storage (GCS)\n \necho &quot;Uploading the generated .sql to Google Cloud Storage (GCS)...&quot;\n \ngsutil cp airbyte_backup.sql gs://$bucket_name/airbyte_db/$current_date/airbyte_backup.sql\n \n  \n \n# Remove the generated .sql file\n \necho &quot;Removing the generated .sql file...&quot;\n \nrm airbyte_backup.sql\n \n  \n \necho &quot;Process completed.&quot;\n \n./run-ab-platform.sh\nIt‚Äôll drop it into your bucket-name in the airbyte_db folder into a folder created with the date of the backup. Please make sure to replace with your GCS bucket name.\nTo run this script automatically you have to do this\n\nOpen the crontab by doing\n\ncrontab -e\n\nAdd this line into the file\n\n0 12 * * * ~/airbyte/backup_script.sh\nthe 0 1 * * * is the cron that I set (in my situation) it runs at 12pm and it‚Äôll run the script at the following location ~/airbyte/backup_script.sh.\nMake sure to put the backup_script.sh in the same folder as your airbyte instance."},"airbyte/how-to-setup-airbyte-gcp":{"title":"How to host Airbyte on Google Cloud Platform?","links":["Airbyte","Docker"],"tags":["Airbyte","GCP"],"content":"In this article I‚Äôll show you how to setup Airbyte on Google Cloud Platform.\nSetup of Airbyte on the VM\nCreate your Virtual Machine\nAccording to Airbyte documentation here the requirements:\n\nTo test Airbyte, we recommend an e2.medium instance and provision at least 30GBs of disk per node\nTo deploy Airbyte in a production environment, we recommend a n1-standard-2 instance\n\nInstall Docker &amp; docker-compose\nInstall docker on your VM\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common\ncurl -fsSL download.docker.com/linux/debian/gpg | sudo apt-key add --\nsudo add-apt-repository &quot;deb [arch=amd64] download.docker.com/linux/debian buster stable&quot;\nsudo apt-get update\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\nsudo usermod -a -G docker $USER\nThen install docker compose\nsudo apt-get -y install docker-compose-plugin\ndocker compose version\nThen close your SSH connection and open it again to take into account the modifications.\nInstall Airbyte\nmkdir airbyte &amp;&amp; cd airbyte\nwget raw.githubusercontent.com/airbytehq/airbyte/master/run-ab-platform.sh\nchmod +x run-ab-platform.sh\n./run-ab-platform.sh -b\nConnection to your Airbyte instance\nNow that you‚Äôve set up Airbyte on your machine and started the instance, you can mirror it but running this command and access it here http://localhost:8000.\ngcloud --project=$PROJECT_ID beta compute ssh $INSTANCE_NAME -- -L 8000:localhost:8000 -N -f"},"caviar-airways":{"title":"Caviar-airways: discover the world through music","links":[],"tags":["html","js","website","music"],"content":"Introduction\nI love music! I know, it‚Äôs not very original since a lot of people spend a lot of their time listening music. With a friend in my business school we did our thesis together by mixing our two passions: music (his) and computer science (mine). This is why we chose the impact of the recommandation algorithms in the music industry.\nTwo years after our graduation, he came to me with an idea. He wanted to create a map with some pin points on it in order to redirect to his Spotify playlists (take a look at his YouTube channel). Unfortunately, he‚Äôs not good at programming so I suggested that I would take the lead of the code and I‚Äôll try to interpret as much as I can how he pictured the website.\nThis idea came from Gatien Tamalet, I built the website for him. If you want to visit the website it‚Äôs here:\ngraceful-clear-kangaroo.glitch.me\nThe born of caviar-airways.com\nFunny thing to mention, I just said my friend is not good at programming (logical since he doesn‚Äôt know how to code), however I don‚Äôt have any professional front-end (neither back-end) skills. I learnt by myself watching some videos on YouTube but I‚Äôm specialized in the data field. This is why the website was built with almost pure HTML, CSS and vanilla JS (I just used jQuery because I was familiar with it for the administration interface).\nThe idea is pretty simple: a map on which you can find points, if the user hovers one of them, the Airport tiles in the header will move to the ‚Äúdestination‚Äù. The name of the playlist and the picture are displayed and if the user clicks on it, he/she will be redirected to the Spotify playlist.\nThe choice of the map technology\nMaybe you heard about it but the Google maps API is not free anymore. Indeed, they gave you $200 credits per month but still you need to provide some banking information. So instead of using OpenStreet Map which is 100% free I decided to try Mapbox. They have quite a generous free-tier (100,000 monthly requests).\nThe choice of the data format\nBased of the documentation of Mapbox I used a .geojson which is quite convenient to use because I was already familiar with .json format. For every point I wanted to create I filled:\n\na unique id\nthe name of the playlist\nthe city that would be displayed in the Airport tiles\nthe link of the Spotify playlist\nthe coordinates with latitude and longitude of the point\nthe image link of the Spotify\n\nThe technology behind the airport tiles\nTo be fully transparent with you I found it on a codepen after searching on Google, it‚Äôs done with pure JavaScript and CSS (codepen.io/chonz0/pen/NGRbWj or codepen.io/artsunique/pen/jOdORNY).\nThe admin interface\nSince I chose to build everything in pure HTML, CSS and JavaScript (except for jQuery library) I decided to create an admin interface from scratch.\nThe interface is quite simple as you can see\n\n  \n  Screenshot of the Admin Homepage\n\nAdd a new point\nYou can add a new point by filling the following information:\n\nlabel: name that will be displayed under the playlist image\ncity: name of the destination for the airport tiles\nlink: link of the playlist\ncoordinates:  latitude &amp; longitude separated by a comma\n\nWhenever you click ‚ÄúSend‚Äù, the playlist will be added to the .geojson and will be displayed on the map.\nFetch covers from Spotify\nThis button is quite simple, it just fetches all the cover images and update them if necessary.\nModification / deletion of a point\nUnder there is a table with all existing playlists. On the right there are two buttons:\n\nThe first one when you click on it you can modify all the fields (except for the image) and you need to click again to save the modifications\n\n\n2. The second one is a deletion button in order to remove a point from the map\n\nConclusion\nThat‚Äôs it! It was a pretty simple and fun project to do. To be honest I would like at some point to update the code base to something more modern (jQuery is so old‚Ä¶). If so, I‚Äôll update this page with details on the new technology used."},"gcp/how-to-delete-old-dbt-models-bigquery":{"title":"How to remove deleted dbt models in BigQuery?","links":[],"tags":["bigquery","dbt","GCP"],"content":"Context\nI‚Äôm pretty sure that you already had renamed or removed a model in dbt and after few weeks/months you noticed that you have the old model and the new one that co-exist in your data warehouse.\nThis is particularly annoying because it can be misleading especially when you aren‚Äôt the owner of the table and you don‚Äôt know which one is deprecated.\nUnfortunately, dbt doesn‚Äôt offer the possibility to remove with just a command the deprecated models. But the good news is that with macros you can find a way to do it.\nTwo macros (or one if you want)\nI created two macros to do it:\n\nlist_deleted_models.sql\n\n{% macro list_deleted_models() -%}\n \n    -- First step, listing all the current_models\n    {%- if execute -%}\n        {%- set current_models = [] -%}\n        {%- for node in graph.nodes.values() -%}\n            {%- do current_models.append(node.name) -%}\n        {%- endfor -%}\n    {%- endif -%}\n \n    -- Extract the table_catalog, schema, table_name and command &#039;BASE + the name of the table&#039;\n    select\n        table_catalog,\n        table_schema,\n        table_name,\n        concat(\n            replace(table_type, &#039;BASE &#039;, &#039;&#039;),\n            concat(\n                &#039; `{{target.database}}.{{target.schema}}&#039;, &#039;.&#039;, concat(table_name, &#039;`&#039;)\n            )\n        ) as command\n    from `region-eu.information_schema.tables` -- Since I have my datasets set on the EU region this is why I use this to list all existing tables\n    where\n        -- I filter all tables that doesn&#039;t exist in my current_models variable\n        upper(table_name) not in (\n            {%- for model in current_models -%}\n                &#039;{{ model.upper() }}&#039; {%- if not loop.last -%}, {% endif -%}\n            {%- endfor -%}\n        )\n        -- I filter by using regex in order to only check models in staging/hub/mart/expo/snapshot/seeds layers to avoid removing raw data\n        and regexp_contains(\n            lower(table_name), r&#039;^(stg|prep|fact|fct|dim|mart|expo|snapshot|seed)_&#039;\n        )\n        and regexp_contains(\n            lower(table_schema), r&#039;_(staging|seeds|mart|hub|expo|snapshots)$&#039;\n        )\n{%- endmacro %}\n\ndelete_old_models.sql\n\n{% macro delete_old_models() -%}\n{% set query = list_deleted_models() %}\n{% set results = run_query(query) %}\n \n{% set drop_commands = [] %}\n \n{# Process the query results #}\n{% if results and results.columns %}\n    {% for row in results.rows %}\n        {% set catalog = row[&#039;table_catalog&#039;] %}\n        {% set dataset = row[&#039;table_schema&#039;] %}\n        {% set table = row[&#039;table_name&#039;] %}\n        \n        {# Construct the DROP TABLE command #}\n        {% set drop_command = &quot;DROP TABLE {}.{}.{}&quot;.format(catalog, dataset, table) %}\n        {% do drop_commands.append(drop_command) %}\n    {% endfor %}\n{% endif %}\n \n{# Execute each DROP TABLE command #}\n{% for command in drop_commands %}\n    {% do run_query(command) %}\n{% endfor %}\n{% endmacro %}\nI split it in two macros to avoid having 1 big macro to maintain but it‚Äôs totally relevant to have only one macro to call.\non-run-end config\nThen I use the on-run-end project config in dbt_project.yml in order to execute the delete_old_models macro at each run.\non-run-end:\n  - &quot;{{ delete_old_models() }}&quot;\nBased on that everyday the removed models are deleted in BigQuery.\nConclusion\nHere a quick method to delete deprecated models for BigQuery but I‚Äôm pretty convinced that it could be done elsewhere.\nSource of the creation of this:\ndiscourse.getdbt.com/t/faq-cleaning-up-removed-models-from-your-production-schema/113"},"gcp/index":{"title":"GCP","links":[],"tags":[],"content":"Here you‚Äôll find all my articles linked to GCP."},"github-actions/ci-cd-github-actions-dbt-core":{"title":"CI/CD pipeline implementation with GitHub Actions for dbt-core transformations","links":[],"tags":["GitHub","GitHub-Actions","dbt-core"],"content":"CI/CD Pipeline Implementation with GitHub Actions for dbt-Core Transformations\nProject Overview\nI built a dbt project for my company to manage data transformations in BigQuery. This project required creating a seamless CI/CD pipeline that would streamline the development process and ensure quality control with every change. To achieve this, I integrated GitHub Actions, automating tests, running modified files only, and maintaining a strict separation between development and production environments.\nEnvironment Setup\nIn BigQuery, I have separated two distinct environments:\n\nDevelopment: Used for testing and running the latest changes.\nProduction: Runs the stable and verified dbt models every day or on release.\n\nHaving two environments ‚Äî development and production ‚Äî in a data warehouse is crucial for ensuring the stability, accuracy, and reliability of data workflows. Here‚Äôs why:\n1. Prevent Disruptions in Production\nThe production environment contains live, critical data that business stakeholders rely on for decision-making. Any errors or changes introduced directly into production could disrupt operations, lead to inaccurate reporting, and damage trust in the data.\nBy separating the environments, you can test new changes, transformations, and queries in the development environment without risking disruptions in production.\n2. Safe Testing and Development\nIn the development environment, data engineers, analytics engineers or data analysts can experiment with new models, transformations, and optimizations. This allows for iterative testing, debugging, and validation of the changes without the fear of breaking live data workflows.\nHaving this space to test ensures that any issues are resolved before code is promoted to production.\n3. Data Quality Assurance\nA key aspect of managing a data warehouse is ensuring the quality and accuracy of the data. By using a development environment to run tests, validate queries, and check for anomalies, you ensure that only validated, high-quality data models are deployed to production.\nIn dbt, for example, you can run tests like unique checks, not-null constraints, and referential integrity checks in development before moving to production.\n4. Controlled Deployment of Changes\nIn a CI/CD workflow, having two environments allows for incremental deployment. Changes can be tested and validated in development, and only those that pass are merged into production. This reduces the likelihood of bugs and errors affecting the production environment.\nWith version control and environment separation, you can deploy changes more confidently, ensuring minimal impact on production.\n5. Auditability and Rollback\nThe two-environment setup provides a clear audit trail of changes. If an issue arises after a release to production, it is easy to trace the modifications made in the development environment and roll back to a stable state if needed.\nHaving a safe place for experimentation allows teams to better manage rollbacks and troubleshoot potential issues.\n6. Efficient Resource Management\nProduction workloads are usually heavier and optimized for performance. Running experimental queries or transformations directly in production could cause resource bottlenecks, slow down critical processes, or increase costs.\nBy isolating development processes, teams can run tests and transformations in a separate, controlled environment without affecting the performance or cost efficiency of the production system.\nGitHub Actions Workflow\nThe CI/CD pipeline uses GitHub Actions to automate the following tasks:\n1. Pull Request (PR) Validation and Testing\nThis workflow, named dev-test-pr, is designed to ensure that pull requests (PRs) meet certain criteria before they can be merged. It includes checks for rebase status, commit count, commit messages, specific code patterns, and SQL file linting using SQLFluff.\nWorkflow: dev-test-pr\nTrigger\nThe workflow is triggered by pull request events such as:\n\nOpening a PR\nSynchronizing (updating) a PR\nReopening a PR\nMarking a PR as ready for review\n\non:\n \npull_request:\n \ntypes: [opened, synchronize, reopened, ready_for_review]\nEnvironment Variables\nSeveral environment variables are set up for use throughout the workflow:\nenv:\n\tDBT_PROFILES_DIR: ./\n\tDBT_GOOGLE_PROJECT: ${{ vars.DBT_GOOGLE_PROJECT }}\n\tDBT_GOOGLE_DATASET: dev_staging\n\tDBT_GOOGLE_DATASET_QA: dev_qa\n\tDBT_GOOGLE_KEYFILE: /tmp/google/google-service-account.json\n\tDBT_MANIFEST: ./manifest.json\n\tDBT_STATE: ./\n\tENV: dev\n\tTARGET: dev\n\tKEYFILE_CONTENTS: ${{ secrets.DBT_SVC_DEV }} # Contents of the keyfile from GitHub Secrets\nJobs\n1. Rebase Check\nThis job checks if the PR branch is rebased on the main branch.\n \njobs:\n \nrebase-check:\n \nif: ${{ ! github.event.pull_request.draft }}\n \nruns-on: ubuntu-latest\n \nsteps:\n \n- name: Checkout PR branch\n \nuses: actions/checkout@v2\n \nwith:\n \nref: ${{ github.event.pull_request.head.ref }}\n \nfetch-depth: 0\n \n  \n \n- name: Fetch main branch\n \nrun: git fetch origin main\n \n  \n \n- name: Check if PR is rebased on main\n \nrun: |\n \ngit merge-base --is-ancestor origin/main ${{ github.event.pull_request.head.sha }}\n \nif [ $? -ne 0 ]; then\n \necho &quot;::error::PR is not rebased on main branch. Please rebase your branch.&quot;\n \nexit 1\n \nfi\n \n2. Commit Count Check\nThis job ensures the PR has only one commit.\n \n  \n \nnb-of-commits-check:\n \nif: ${{ ! github.event.pull_request.draft }}\n \nruns-on: ubuntu-latest\n \nsteps:\n \n- name: Checkout PR branch\n \nuses: actions/checkout@v2\n \nwith:\n \nref: ${{ github.event.pull_request.head.ref }}\n \nfetch-depth: 0\n \n  \n \n- name: Count commits in PR\n \nid: commit-count\n \nrun: |\n \nCOMMIT_COUNT=$(git rev-list --count origin/main..HEAD)\n \necho &quot;commit_count=$COMMIT_COUNT&quot; &gt;&gt; $GITHUB_ENV\n \n  \n \n- name: Check commit count\n \nrun: |\n \nif [ &quot;${{ env.commit_count }}&quot; -gt 1 ]; then\n \necho &quot;::error::PR has more than 1 commit. Please squash your commits.&quot;\n \nexit 1\n \nfi\n \n3. Commit Message Check\nThis job validates that commit messages follow specified conventions.\n \ncommit-message-check:\n \nif: ${{ ! github.event.pull_request.draft }}\n \nruns-on: ubuntu-latest\n \nsteps:\n \n- name: Checkout PR branch\n \nuses: actions/checkout@v2\n \nwith:\n \nref: ${{ github.event.pull_request.head.ref }}\n \nfetch-depth: 0\n \n  \n \n- name: Check commit messages\n \nrun: |\n \nINVALID_COMMIT_MSG=$(git log --format=%B origin/main..HEAD | grep -vE &#039;^(feat|ci|fix|docs|refactor|creation)&#039;)\n \nif [ -n &quot;$INVALID_COMMIT_MSG&quot; ]; then\n \necho &quot;::error::Some commits do not start with a valid keyword (feat, ci, fix, docs, refactor, creation): $INVALID_COMMIT_MSG&quot;\n \nexit 1\n \nfi\n \n4. SQLFluff Lint\nThis job lints modified SQL files using SQLFluff to ensure code quality.\n \nsqlfluff-lint:\n \nruns-on: ubuntu-latest\n \nsteps:\n \n- name: Checkout Code\n \nuses: actions/checkout@v3\n \nwith:\n \nfetch-depth: 0\n \n  \n \n- name: Fetch base branch\n \nrun: git fetch origin ${{ github.event.pull_request.base.ref }}\n \n  \n \n- name: Set up Python and Install SQLFluff\n \nuses: actions/setup-python@v4\n \nwith:\n \npython-version: &#039;3.11&#039;\n \n- run: |\n \npython -m pip install --upgrade pip\n \npip install sqlfluff\n \n  \n \n- name: Get Modified SQL Files\n \nid: sql_files\n \nrun: |\n \nCHANGED_FILES=$(git diff --name-only --diff-filter=ACM origin/${{ github.event.pull_request.base.ref }})\n \nSQL_FILES=$(echo &quot;$CHANGED_FILES&quot; | grep -E &#039;\\.sql$&#039; || true)\n \nSQL_FILES=$(echo &quot;$SQL_FILES&quot; | tr &#039;\\n&#039; &#039; &#039; | xargs)\n \necho &quot;SQL_FILES=$SQL_FILES&quot; &gt;&gt; $GITHUB_ENV\n \n  \n \n- name: Run SQLFluff Lint\n \nif: env.SQL_FILES &amp;&amp; env.SQL_FILES != &#039;&#039;\n \nrun: |\n \nif [ -z &quot;$SQL_FILES&quot; ]; then\n \necho &quot;No SQL files to lint.&quot;\n \nexit 0\n \nfi\n \nsqlfluff lint $SQL_FILES\n \nWorkflow Benefits\n\n\nAutomated Quality Checks: The workflow ensures that PRs meet predefined standards, reducing the chance of errors in production.\n\n\nRebase Enforcement: Prevents issues that can arise from outdated branches.\n\n\nCommit Message Standards: Maintains consistency in commit messages, improving project documentation and history.\n\n\nCode Quality Assurance: Linting SQL files promotes best practices in SQL code writing.\n\n\n2. Execute dbt build in development environment on PR Merge\nThis GitHub Actions workflow automates the process of executing dbt build in development environment whenever a pull request (PR) is merged. It ensures that the project is built and deployed correctly, while also managing authentication and logging.\nWorkflow Details\n\n\nName: run dev on PR merge\n\n\nTrigger: The workflow runs when a pull request is closed.\n\n\nEnvironment Variables\nThe workflow defines several environment variables to facilitate its operations:\n\n\nDBT_PROFILES_DIR: Specifies the directory for dbt profiles.\n\n\nDBT_GOOGLE_PROJECT: The Google project ID.\n\n\nDBT_GOOGLE_DATASET: The dataset to use for development.\n\n\nDBT_GOOGLE_KEYFILE: Path for the Google service account keyfile.\n\n\nDBT_MANIFEST: Path to the dbt manifest file.\n\n\nDBT_STATE: Directory for dbt state.\n\n\nENV: Environment setting (set to dev).\n\n\nTARGET: Target environment (set to dev).\n\n\nKEYFILE_CONTENTS: Contents of the Google service account keyfile pulled from GitHub Secrets.\n\n\nJob\nThis job runs on an ubuntu-latest environment and consists of several key steps:\n1. Checkout Code\nUses actions/checkout@v4 to check out the code from the repository.\n2. Prepare Google Keyfile\nCreates the necessary directory and writes the keyfile contents.\n3. Authenticate to Google Cloud\nUses the google-github-actions/auth@v2 action to authenticate using the service account.\n4. Download Manifest File\nDownloads the manifest.json file from Google Cloud Storage using gsutil.\n5. Run a Multi-line Script\nPlaceholder for additional build, test, and deployment commands.\n6. Set Up Python\nUses actions/setup-python@v2 to set up Python 3.11.\n7. Install Dependencies\nInstalls necessary Python packages, including dbt-core and dbt-bigquery.\n8. Run dbt Build\nExecutes dbt build to build the project based on the modified states.\n9. Upload Logs on Failure\nIf the build fails, it uploads logs to Google Cloud Storage for troubleshooting.\n10. Deploy via GCS\nIf successful, it deploys the manifest.json file to Google Cloud Storage.\n11. Report Status on Slack\nUses a Slack notification action to report the status of the workflow, including details about the run and the user who initiated the PR.\nWorkflow Benefits\nThis GitHub Actions workflow effectively automates the development process for merged pull requests, ensuring that all necessary tasks are performed consistently and efficiently, while also keeping the team informed about the workflow‚Äôs status.\n3. Production Release (prod) - Incremental Runs\nIn this GitHub Actions workflow, I automated the process of running dbt whenever a new release is published (only for modified models). Here‚Äôs a detailed breakdown of the steps involved and their purposes:\n1. Trigger on Release Publication\nThe workflow is triggered by the release event, specifically when a new release is published.\nThis ensures that dbt tasks are executed automatically whenever a new version of your project is released.\n2. Setting Up Environment Variables\nThe workflow sets several environment variables, including:\n\n\nDBT_PROFILES_DIR: Specifies where the dbt profiles are located.\n\n\nDBT_GOOGLE_PROJECT and PROJECT_ID: Point to the Google Cloud project that will be used for dbt‚Äôs BigQuery integration.\n\n\nDBT_GOOGLE_KEYFILE: A temporary path where the service account credentials will be stored.\n\n\nKEYFILE_CONTENTS: Fetches the contents of the service account key from the GitHub Secrets.\n\n\n3. Job: run-dbt\nRuns on ubuntu-latest: The job executes on the latest Ubuntu environment.\nSteps within the run-dbt job:\n3.1. Checkout Repository\nUses actions/checkout@v3 to pull the code from the repository. The fetch-depth: 0 option ensures all tags are retrieved, which is important for later comparisons between releases.\n3.2. Setup for Google Cloud Authentication\n\n\nCreates a directory for storing the Google Cloud service account credentials, then writes the KEYFILE_CONTENTS into the DBT_GOOGLE_KEYFILE.\n\n\nThe step google-github-actions/auth@v2 is used to authenticate the GitHub Actions runner with Google Cloud, using the provided credentials.\n\n\n3.3. Google Cloud Storage Interaction\n\n\nThe gcloud auth activate-service-account step authenticates with Google Cloud.\n\n\nA gsutil command downloads the manifest.json file from Google Cloud Storage. This file is used for state-based dbt builds to compare between the current and previous dbt runs.\n\n\n3.4. Python Setup\n\n\nactions/setup-python@v2 installs Python version 3.11 for running dbt. This is done twice to ensure Python and dbt are properly set up.\n\n\n3.5. Install dbt and Dependencies\n\n\nInstalls dbt-core and dbt-bigquery to allow running dbt with BigQuery as the target.\n\n\ndbt deps ‚Äîtarget prod installs dbt project dependencies using the prod target environment.\n\n\n3.6. Run dbt Build\n\n\nThe dbt build command runs dbt models that have been modified based on the state comparison (‚Äîselect state:modified+). This allows only the modified models and their dependencies to be rebuilt, optimizing runtime.\n\n\n3.7. Generate dbt Documentation\n\n\nThe dbt docs generate step builds documentation from the dbt project, which will be used to display metadata and lineage for the models in your dbt project.\n\n\n3.8. Custom Python Script\n\n\nA custom Python script replace_dbt_docs.py is executed. It modifies the generated dbt documentation before it‚Äôs uploaded to the cloud.\n\n\n3.9. Upload dbt Docs to Google Cloud\n\n\nUses gsutil to upload the dbt documentation (index.html, catalog.json, manifest.json) to a specified Google Cloud Storage bucket (gs://lucca-dbt-docs/static/).\n\n\n4. Capture and Categorize File Changes\nThis step is responsible for comparing the current release with the previous one, capturing which files have been modified, added, deleted, or renamed.\nA git diff command is used to list the differences between the two release tags.\nThe changes are categorized and stored in separate text files (modified_files.txt, added_files.txt, deleted_files.txt, renamed_files.txt), then compiled into a summary file (file_changes.txt).\n5. Slack Notification\nAfter capturing the file changes, the workflow sends the categorized changes to a Slack channel.\nTwo Slack notifications are triggered:\n\n\nStatus Report: This sends the overall status of the workflow (e.g., success or failure) to a Slack channel.\n\n\nFile Changes Report: This sends the categorized file changes between the two releases (added, modified, deleted, renamed) to a designated Slack channel, keeping the team informed about what has changed in the latest release.\n\n\nKey Benefits\n\n\nAutomated dbt Runs on Release: Ensures that the latest version of your data models is built and documented whenever a new release is published.\n\n\nSelective Rebuilds: By using dbt build ‚Äîselect state:modified+, only the modified models are rebuilt, reducing build times and ensuring efficient processing.\n\n\nContinuous Deployment of Documentation: dbt documentation is generated and pushed to a cloud storage bucket, allowing the latest documentation to be accessible immediately after each release.\n\n\nAutomated Notifications: Slack notifications keep the team updated on the workflow status and changes, improving communication and transparency in the release process.\n\n\nFile Change Tracking: The workflow tracks and categorizes file changes between releases, which helps with auditing and understanding the scope of each release.\n\n\n4. Daily Production Runs\n\n\nEvery day, the production environment runs a complete refresh of the dbt models to ensure data consistency and accuracy.\n\n\nKey Benefits\n\n\nFaster Development Cycles: By running only the modified files, we significantly reduced the time to test and deploy changes.\n\n\nAutomated Quality Checks: PR validation with sqlfluff and dbt testing ensures that only high-quality code is merged.\n\n\nSeparation of Environments: The development and production environments in BigQuery are completely isolated, minimizing the risk of impacting production data during development.\n\n\nContinuous Production Runs: The daily production runs ensure that data is always up to date and accurate, providing business stakeholders with reliable insights.\n\n\nConclusion\nThis CI/CD pipeline has streamlined the development process for dbt transformations in BigQuery, reduced time spent on deployments, and improved overall code quality. By leveraging GitHub Actions and dbt‚Äôs manifest, I was able to create a robust system that scales well and maintains data integrity across environments.\nFeel free to reach out if you‚Äôd like a similar solution implemented for your data workflows!"},"index":{"title":"Welcome to my engineering blog","links":[],"tags":[],"content":"Who am I?\nHello üëã, I‚Äôm Pierre Munhoz, a Senior Data Engineer with 6 years of experience. I enjoy cooking, running &amp; coding (especially when it involves playing with data).\nI began my career as a Data Analyst at Contentsquare and currently work as a Data Engineer at Lucca. For more information about my work and projects, please visit my portfolio.\nAlthough I graduated from a business school, I consider myself more of an engineer. My business education, however, has been invaluable in understanding how to balance technical decisions with business objectives, which I believe is crucial.\nThe purpose of this website\nAs a teenager, I felt a strong urge to share my knowledge through videos and tutorials. What began with Minecraft plugins and server creation has evolved into a passion for analytics and data engineering.\nI often find it challenging to capture all my experiences and achievements in a resume, which can be frustrating. Through this website, I aim to showcase my skills and demonstrate what I‚Äôm capable of."},"interviews/case-study":{"title":"case-study","links":[],"tags":[],"content":""}}